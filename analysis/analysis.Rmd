---
title: "Bo-NO-bouba-kiki: Picture-word mapping but not sound symbolism in a language trained bonobo"
subtitle: "Supplementary material"
author: 
  - "Konstantina Margiotoudi"
  - "Manuel Bohn"
  - "Natalie Schwob"
  - "Jared Taglialatela"
  - "Friedemann Pulvermüller"
  - "Amanda Epping"
  - "Ken Schweller"
  - "Matthias Allritz"
output: 
  bookdown::pdf_document2:
    toc: false 
    number_sections: false    
  bookdown::html_document2:
    toc: yes
    toc_float: true
    fig_caption: yes
    #code_folding: hide
    number_sections: false
#bibliography: library.bib
#header-includes:
#  \usepackage{caption}
#  \renewcommand{\figurename}{Supplementary Figure}
#  \renewcommand{\tablename}{Supplementary Table}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, size="small")

library(tidyverse)
library(brms)
library(broom)
library(ggthemes)
library(tidyboot)
library(ggpubr)
library(tidybayes)
library(coda)
library(knitr)
library(ggbreak)

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```


```{r, include=F}
data <- read_csv("../data/data.csv")
```

# Data and code availability

All data and code associated with the study can be found in the following online repository: https://github.com/manuelbohn/bonoboubakiki

# Power analysis

We conducted a power analysis to specify the number of test trials in the experiment. The details of this analysis can be found in the file [`/analysis/power_analysis.Rmd](https://github.com/manuelbohn/bonoboubakiki/blob/master/analysis/power_analysis.Rmd). The results of this power analysis suggested that when assuming a true effect of 65% correct responses with 300 test trials our model would support the conclusion that performance was above chance in 88/100 cases (see Figure \@ref(fig:simres)).

```{r, cache = T}
sim_res_300_p65 <-readRDS("saves/sim_300_p65.rds")

sim_res_int_p65 <- sim_res_300_p65 %>%
  filter(term == "b_Intercept")%>%
  mutate(estimate = plogis(estimate),
         lower = plogis(lower),
         upper = plogis(upper), 
         inf = ifelse(lower<.5, "true", "false"))
```

```{r simres, fig.height = 3, fig.cap = "Power analysis based on 100 simulations with a true effect of 65\\% correct responses and 300 test trials. Points show the model intercept for each simulation (with with 95\\% CrI). Simulations in red yielded a wrong conclusion.", out.width="100%"}

ggplot(sim_res_int_p65, aes(x = seed, y = estimate, ymin = lower, ymax = upper, col = inf)) +
  geom_hline(yintercept = 0.5, color = "black", lty = 2) +
  ylim(0,1)+
  geom_pointrange(fatten = 1/2) +
  labs(x = "Simulation",
       y = "Proportion correct")+
  scale_x_continuous(breaks = c(1,100))+
  scale_color_manual(values = c("black", "firebrick"))+
  guides(col = F)+
  theme_minimal()
```

# Descriptives

## Number of Trials

Table \@ref(tab:trialtab) gives the number of trials per condition. There were 30 sessions in total, each with 10 test trials. Because of a software error, two regular trials were missing in session 1 and 2 (see main paper for detail). 

```{r trialtab}
t1 <- data %>%
  group_by(condition)%>%
  summarise(n = n())

knitr::kable(t1, caption = "Number of trials per condition.", digits = 2, align = "l")
```

# Reaction times

Figure \@ref(fig:rtplot) visualizes the reaction times by correct choice and trial type. 

```{r rtplot, fig.height = 3, fig.cap = "Reactions times for regular and test trials split by correct and incorrect responses.", out.width="100%"}
rt_data <- data%>%
  mutate(correct = ifelse(correct == 1, "correct", "incorrect"))

rt_plot <- ggplot(rt_data, aes(x = RT, col = condition, fill = condition, lty = correct))+
  geom_density(alpha = .3)+
  xlim(0,6000)+
  xlab("Reaction Time (ms)")+
  ylab("")+
  scale_color_colorblind(name = "Condition")+
  scale_fill_colorblind(name = "Condition")+
  scale_linetype_discrete(name = "Choice")+
  theme_minimal()+
  theme(axis.text.y = element_blank(),
        axis.ticks.y  = element_blank(),
        legend.position = c(0.7,0.6),
        legend.direction = "vertical",
        legend.box = "horizontal")

rt_plot
```

```{r}
rt_data_model <- data %>%
  mutate(z_trial = scale(cont_trial))

bm_rt<- brm(RT ~ condition* z_trial + (z_trial | SampleSound ),
          data = rt_data_model,
          family = shifted_lognormal(),
          cores = 4,
          chains = 4,
          iter = 4000,
          control = list(adapt_delta = 0.95))

saveRDS(bm_rt, "saves/model_rt.rds")

bm_rt <-readRDS("saves/model_rt.rds")

```

Figure \@ref(fig:rtrial) shows the change in reaction time over time. Reaction times are relatively stable across time and changed in similar ways in regular and test trials.

```{r rtrial, fig.height = 3, fig.cap = "Reactions time across trials (continous across sessions) for regular and test trials. Light dots represent individual trials. Regression line gives a smoothed conditional mean.", out.width="100%"}
rt_data%>%
  group_by(condition)%>%
  mutate(trial_cont = 1:length(trial))%>%
  ggplot(aes(x = trial_cont, y = RT, col = condition))+
  geom_point(alpha = .25)+
  geom_smooth(col = "firebrick", method = "loess")+
  facet_grid(~condition, scales = "free_x")+
  labs(x = "Trial (continous across sessions)", y = "Reaction time (ms)")+
  scale_color_colorblind()+
  guides(col = F)+
  theme_minimal()
```

```{r rtmodel, fig.cap = "Estimates for condition, trial and interaction between trial and condition (with 95\\% CrI) in the model for reaction times. Dashed line shows chance level. Solid black line highlights a break in the x-axis", out.width="100%", fig.height = 4}
bm_rt%>%
  spread_draws(b_Intercept, b_conditiontest, b_z_trial, `b_conditiontest:z_trial`)%>%
  rename(Intercept = b_Intercept,
         `Condition (test trials)` = b_conditiontest,
         Trial = b_z_trial,
         `Condition (test trials) * trial` = `b_conditiontest:z_trial`)%>%
  pivot_longer(names_to = "Estimate", values_to = "Value", cols = c(Intercept, `Condition (test trials)`, Trial,`Condition (test trials) * trial`))%>%
  ggplot(aes(x = Value, y = Estimate)) +
  geom_vline(xintercept = 6.6, size = 2) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  stat_halfeye(alpha = .7)+
  labs(x = "", y = "")+
  scale_x_break(c(0.1, 6.6), scales = 1, ticklabels = c(6.6, 6.7,6.8, 6.9,7, 7.1, 7.2, 7.3, 7.4))+
  #facet_wrap(~Estimate, scales = "free")+
  #scale_fill_manual(values = c("gray80", "firebrick"), name = "95% Credible intervall", labels = c("< 2.5% | > 97.5%","2.5% - 97.5%"))+
  guides(fill = F)+
  theme_minimal()+
  theme(axis.title.x = element_blank())
```

This impression was confirmed in a linear mixed model in which we predicted reaction time by condition and trial. The model had the following structure: 

`RT ~ condition* z_trial + (z_trial | SampleSound )`

Because reaction times tend to be skewed and have an offset, we modeled the response as a `shifted_lognormal` distribution. Figure \@ref(fig:rtmodel) visualizes the model estimates, showing a negative effect of condition and a small negative effect of trial but no interaction between condition and trial. Thus, reaction times were generally faster in test trials compared to regular trials, but they did not change differently over time for the two trial types.

# Results

```{r}
test_data <- data %>%
  filter(condition == "test")%>%
  mutate(z_trial = scale(cont_trial))

regular_data <- data %>%
  filter(condition == "regular")%>%
  mutate(z_trial = scale(cont_trial))
```

## Test trials 

### Comparison to chance

We used a Bayesian generalized linear mixed model with a logit link to analyze performance. Our model hat the following structure: 

`correct ~ 1 + (z_trial | SampleSound )`

In this a model, the intercept models the average rate of correct responses. A value of 0 in link space corresponds to a proportion of correct choices of 0.5. Thus, we inferred that if the 95% credible interval (CrI) around the posterior estimate for the intercept did not include 0, performance was reliably above chance. Figure \@ref(fig:ttmodel) visualizes the posterior distribution based on our model. It shows that the 95%CrI overlaps with 0. We therefore concluded that Kanzi's performance in test trials was not reliably above chance. 

```{r, cache = T}
# model takes a minute or two to initialize and run.
# load rds file if you don't want to wait or if you want to reproduce the exact numbers in the manuscript

# bm_test <- brm(correct ~ 1 + (z_trial | SampleSound ), # + (z_trial | shape_combination),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))

#saveRDS(bm_test, "saves/model_test.rds")

bm_test <-readRDS("saves/model_test.rds")
```

```{r ttmodel, fig.cap = "Estimate for intercept term (with 95\\% CrI) in the model for test trials. Dashed line shows chance level (in link space).", out.width="100%", fig.height = 2}
bm_test%>%
  spread_draws(b_Intercept)%>%
  ggplot(aes(x = b_Intercept, fill = stat(x < -0.08 | x > 0.41))) +
  stat_halfeye(alpha = .7)+
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "Model intercept", y = "Density")+
  scale_fill_manual(values = c("gray80", "firebrick"), name = "95% Credible intervall", labels = c("< 2.5% | > 97.5%","2.5% - 97.5%"))+
  guides(fill = F)+
  theme_minimal()
```

#### Prior sensitivity analysis

The model reported above used default priors as implemented in the `brms` package. Here we explore the effect of different prior distributions for the intercept estimate on the results. The default prior in `brms` is `student_t(3, 0, 2.5)`. We compare this to three other priors. A weak prior `normal(0, 10)`, a medium prior `normal(0, 5)`, and a strong prior `normal(0, 1)`. Figure \@ref(fig:priorsens) visualizes the estimates given the different prior distributions and shows that the prior settings had little to no influence on the posterior estimate in the model. 

```{r}
# weak_prior <- prior(normal(0, 10), class="Intercept")
# medium_prior <- prior(normal(0, 5), class="Intercept")
# strong_prior <- prior(normal(0, 1), class="Intercept")
# weak_effect_prior <- prior(normal(1, 10), class="Intercept")
# medium_effect_prior <- prior(normal(1, 5), class="Intercept")
# strong_effect_prior <- prior(normal(1, 1), class="Intercept")
# 
# bm_test_weak_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = weak_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_weak_prior, "saves/model_test_weak_prior.rds")
# 
# bm_test_weak_prior <-readRDS("saves/model_test_weak_prior.rds")
# 
# bm_test_medium_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = medium_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_medium_prior, "saves/model_test_medium_prior.rds")
# 
# bm_test_medium_prior <-readRDS("saves/model_test_medium_prior.rds")
# 
# bm_test_strong_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = strong_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_strong_prior, "saves/model_test_strong_prior.rds")
# 
# bm_test_strong_prior <-readRDS("saves/model_test_strong_prior.rds")
# 
# # Effect prior
# 
# bm_test_weak_effect_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = weak_effect_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_weak_effect_prior, "saves/model_weak_effect_prior.rds")
# 
# bm_test_weak_effect_prior <-readRDS("saves/model_weak_effect_prior.rds")
# 
# bm_test_medium_effect_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = medium_effect_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_medium_effect_prior, "saves/model_medium_effect_prior.rds")
# 
# bm_test_medium_effect_prior <-readRDS("saves/model_medium_effect_prior.rds")
# 
# bm_test_strong_effect_prior <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           prior = strong_effect_prior,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_strong_effect_prior, "saves/model_strong_effect_prior.rds")
# 
# bm_test_strong_effect_prior <-readRDS("saves/model_strong_effect_prior.rds")
# 
# prior_plot <- bind_rows(
#   fixef(bm_test)%>%as_tibble()%>%mutate(Prior = "default", Effect = "No effect"),
#   fixef(bm_test_weak_prior)%>%as_tibble()%>%mutate(Prior = "weakly informative", Effect = "No effect"),
#   fixef(bm_test_medium_prior)%>%as_tibble()%>%mutate(Prior = "informative", Effect = "No effect"),
#   fixef(bm_test_strong_prior)%>%as_tibble()%>%mutate(Prior = "highly informative", Effect = "No effect"),
#   fixef(bm_test_weak_prior)%>%as_tibble()%>%mutate(Prior = "weakly informative", Effect = "Strong effect"),
#   fixef(bm_test_medium_prior)%>%as_tibble()%>%mutate(Prior = "informative", Effect = "Strong effect"),
#   fixef(bm_test_strong_prior)%>%as_tibble()%>%mutate(Prior = "highly informative", Effect = "Strong effect")
# )

saveRDS(prior_plot, "saves/prior_sensitivity.rds")

prior_plot <- readRDS("saves/prior_sensitivity.rds")
```

```{r priorsens, fig.cap = "Estimate for intercept term (with 95\\% CrI) in the model for test trials with different prior distributions. Dashed line shows chance level (in link space).", out.width="100%", fig.height = 2}

prior_plot%>%
  mutate(Prior = factor(Prior, levels = c("default", "weakly informative","informative","highly informative")))%>%
  ggplot(aes(x = Prior, y = Estimate))+
  geom_hline(yintercept = 0, lty = 2, alpha = .75)+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5))+
  facet_grid(~Effect)+
  labs(y = "Model intercept")+
  ylim(-0.5, 0.5)+
  theme_minimal()
```

#### Subset analysis

```{r}
sel_words = c("keke","kiki","pepe","pipi","lulu","lolo","mumu","momo","nono","nunu")

test_data_sel <- test_data%>%
  mutate(SampleSound = substr(SampleSound, 1, nchar(SampleSound)-4))%>%
  filter(SampleSound %in% sel_words)
```

In McCormick et al. (2015), some pseudo words were rated as more sound symbolic than others. These words were: `keke`,`kiki`,`pepe`,`pipi`,`lulu`,`lolo`,`mumu`,`momo`,`nono`,`nunu`. Given that these words are perceived as more sound symbolic in humans, Kanzi's performance might have been higher in trials with just these words. We therefore re-ran the model on a subset of the test trials that included only these words. This reduces the number of trials to `r length(test_data_sel$SampleSound)`. Figure \@ref(fig:ttsel) shows that the average rate of correct responses was the same when estimated based on trials with only highly sound symbolic pseudo words compared to all trials.

```{r}
# 
# bm_test_sel <- brm(correct ~ 1 + (z_trial | SampleSound ),
#           data = test_data_sel,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))
# 
# saveRDS(bm_test_sel, "saves/model_test_sel.rds")
# 
# bm_test_sel <-readRDS("saves/model_test_sel.rds")
# 
# sel_plot <- bind_rows(
#   fixef(bm_test)%>%as_tibble()%>%mutate(Data = "all trials"),
#   fixef(bm_test_sel)%>%as_tibble()%>%mutate(Data = "subset")
# )
# 
# saveRDS(sel_plot, "saves/sel_plot.rds")

sel_plot <- readRDS("saves/sel_plot.rds")

```

```{r ttsel, fig.cap = "Estimate for intercept term (with 95\\% CrI) in the model for test trials based on all trials (left) and a subset of trials with words that had high sound symbolic ratings in McCormick et al. (2015). Dashed line shows chance level (in link space).", out.width="100%", fig.height = 2}

sel_plot%>%
  ggplot(aes(x = Data, y = Estimate))+
  geom_hline(yintercept = 0, lty = 2, alpha = .75)+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5))+
  labs(y = "Model intercept")+
  ylim(-0.7, 0.7)+
  theme_minimal()
```

### Shape preference

For humans, it has been repeatedly found that sound symbolism effects are stronger for round shapes compared to edgy shapes. We therefore ran a model estimating the rate of correct responses on test trials with round compared to edgy shapes. This model had the following structure: 

`correct ~ test_shape + (z_trial | SampleSound )`

Figure \@ref(fig:tshape) shows that while performance was slightly higher with round compared to edgy shapes, it was nevertheless not reliably different from a level expected by chance. 

```{r, cache = T}
# load .rds file if you don't want to run the model

# bm_shape <- brm(correct ~ test_shape + (z_trial | SampleSound ), #+ (z_trial | shape_combination),
#           data = test_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           iter = 5000,
#           control = list(adapt_delta = 0.95))

#saveRDS(bm_shape, "saves/model_shape.rds")

bm_shape <-readRDS("saves/model_shape.rds")

```

```{r tshape, fig.cap = "Posterior estimates (with 95\\% CrI) for test trials with round or edgy shapes. Dashed line shows chance level (in link space).", out.width="100%", fig.height=3}
bm_shape%>%
  spread_draws(b_Intercept,b_test_shaperound)%>%
  mutate(edgy = b_Intercept,
         round = b_Intercept + b_test_shaperound)%>%
  select(round, edgy)%>%
  gather(test_shape, value)%>%
  ggplot(aes(x = value, y = test_shape)) +
  stat_halfeye(alpha = .7)+
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "Model estimate", y = "Shape")+
  scale_fill_manual(values = c("gray80", "firebrick"), name = "95% Credible intervall", labels = c("< 2.5% | > 97.5%","2.5% - 97.5%"))+
  guides(fill = F)+
  theme_minimal()
```

## Regular trials

### Comparison to chance

Test trials were embedded in regular trials in which Kanzi had to match a word to a picture (cropped and converted to gray scale). We analysed these regular trials in the same way as the test trials using the following model: 

`correct ~ 1 + (z_trial | SampleSound )`

Figure \@ref(fig:rtmodel) shows that the posterior estimate for the intercept was clearly positive and the 95% CrI did not overlap with 0. Thus, Kanzi was clearly able to matcht the sounds to the correct pictures. 

```{r, cache = T}
# model takes long time to run

# bm_regular <- brm(correct ~ 1 + (z_trial | SampleSound ),# + (z_trial | shape_combination),
#           data = regular_data,
#           family = bernoulli(),
#           cores = 4,
#           chains = 4,
#           iter = 5000,
#           control = list(adapt_delta = 0.99, max_treedepth = 20))

#saveRDS(bm_regular, "saves/model_regular.rds")

bm_regular <- readRDS("saves/model_regular.rds")
```

```{r rtmodel, fig.cap = "Estimate for intercept term (with 95\\% CrI) in the model for regular trials. Dashed line shows chance level (in link space).", out.width="100%", fig.height = 2}

bm_regular%>%
  spread_draws(b_Intercept)%>%
  ggplot(aes(x = b_Intercept, fill = stat(x < 1.71 | x > 2.30))) +
  stat_halfeye(alpha = .7)+
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = "Model intercept", y = "Density")+
  scale_fill_manual(values = c("gray80", "firebrick"), name = "95% Credible intervall", labels = c("< 2.5% | > 97.5%","2.5% - 97.5%"))+
  guides(fill = F)+
  theme_minimal()
```

## Results by word

Figure \@ref(fig:resw) shows the proportion of correct responses for each word used in regular and test trials. Each regular word appeared between 26 and 28 times and each sound symbolic test word appeared 15 times.

```{r resw, cache = T,  fig.cap="Proportion of correct responses (with 95\\% confidence interval) for each word for regular and test trials.", out.width="100%", fig.height=12}

data%>%
  mutate(SampleSound = substr(SampleSound, 1, nchar(SampleSound)-4),
         SampleSound = tolower(SampleSound))%>%
  group_by(condition,SampleSound)%>%
  tidyboot_mean(col = correct)%>%
  mutate(SampleSound = fct_reorder(SampleSound, desc(mean))) %>%
  ggplot(aes(x = mean, y =  SampleSound, col = condition))+
  geom_vline(xintercept = .5, lty = 2, alpha = .75)+
  geom_pointrange(aes(xmax = ci_upper, xmin = ci_lower, col = condition),pch = 5)+
  facet_grid(condition~., scales = "free_y", space='free')+
  theme_minimal()+
  labs(x="Proportion correct", y= "Word")+
  xlim(0,1)+
  guides(col = F)+
  #theme(axis.text.x=element_text(angle = 45, vjust = 1, hjust = 1))+
  scale_color_colorblind()
  
```


# Visualizations for manuscript

Figure \@ref(fig:msfig) reproduces Figure 2 from the main manuscript.

```{r, cache = T}
# data from individual sessions
plot_session <- data%>%
  group_by(session,condition)%>%
  tidyboot_mean(col = correct)

# model estimates
model_est <- bind_rows(
  fixef(bm_test)%>%as_tibble()%>%mutate(condition = "test"),
  fixef(bm_regular)%>%as_tibble()%>%mutate(condition = "regular"),
)%>%
  mutate(mean = plogis(Estimate),
         uci = plogis(Q2.5),
         lci = plogis(Q97.5))
  

res_plot <- ggplot()+
  geom_hline(yintercept = 0.5, lty = 2)+
  geom_jitter(data = plot_session, aes(x= condition, y = mean, col = condition), alpha = 0.2, width = 0.1, height = 0)+
  geom_pointrange(data = model_est, aes(x = condition, y = mean, ymax = uci, ymin = lci, col = condition),pch = 5)+
  ylim(0,1)+
  ylab("Proportion correct")+
  xlab("Condition")+
  scale_color_colorblind(name = "Condition")+
  guides(col = F)+
  theme_minimal()

sess_plot <- ggplot(plot_session, aes(x = session, col = condition))+
  geom_hline(yintercept = 0.5, lty = 2)+
  geom_point(aes(y = mean))+
  #geom_pointrange(aes(y = mean, ymax = ci_upper, ymin = ci_lower), pch = 5)+
  geom_line(aes(y= mean, col = condition))+
  ylim(0,1)+
  #facet_grid(condition~.)+
  ylab("Prop. correct")+
  xlab("Session")+
  scale_color_colorblind(name = "Condition")+
  theme_minimal()+
  theme(legend.position = c(0.5,0.1),
        legend.direction = "horizontal")
```

```{r, cache=T}
plot1 <- data%>%
  filter(condition == "test")%>%
  group_by(session, test_shape)%>%
  summarise(mean = mean(correct))

shape_est <- bm_shape%>%
  spread_draws(b_Intercept,b_test_shaperound)%>%
  mutate(edgy = b_Intercept,
         round = b_Intercept + b_test_shaperound)%>%
  select(round, edgy)%>%
  gather(test_shape, value)%>%
  group_by(test_shape)%>%
  summarise(mean = mean(value),
            uci = hdi_upper(value),
            lci = hdi_lower(value))%>%
  mutate_if(is.numeric, plogis)


shape_plot <- ggplot()+
  geom_hline(yintercept = 0.5, lty = 2)+
  geom_jitter(data = plot1, aes(x= test_shape, y = mean, col = test_shape), alpha = 0.2, width = 0.1, height = 0)+
  geom_pointrange(data = shape_est, aes(x = test_shape, y = mean, ymax = uci, ymin = lci, col = test_shape), pch = 5)+
  #geom_line(aes(y= mean, col = shape))+
  ylim(0,1)+
  ylab("Prop. correct")+
  xlab("Shape")+
  scale_color_ptol()+
  guides(col = F)+
  theme_minimal()
```

```{r msfig, fig.cap="(A) Proportion of correct choices in regular and test trials. (B) Distribution of reaction times in regular and test trials for correct and incorrect choices. (C) Proportion of correct choices in each session for regular and test trials. (D) Proportion of correct choices in test trials with either edgy or round target shapes. In A and D: Diamonds and error bars represent the mean and 95\\% CrI based on the models’ posterior distribution. Transparent dots show horizontally jittered session means of the data.", out.width="100%", fig.height=4, fig.width = 10}

ggarrange(
  
  res_plot,
  
  ggarrange(
    rt_plot,
    
    ggarrange(
    sess_plot,
    shape_plot,
    nrow = 1,
    ncol = 2,
    widths = c(1.5,1),
    labels = c("C","D")
    ),
    nrow = 2,
    ncol = 1,
    labels = c("B","")),
  nrow = 1,
  ncol = 2,
  labels = c("A",""),
  widths = c(1,1.3)
)

#ggsave("figures/results.png", height = 3, width = 7, scale = 1.4)
```



